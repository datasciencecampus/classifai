{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add your own pre and post processing logic with <i>Hooks</i>ü™ù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ClassifAI package uses a defined data flow process to ensure that all the modules work together:\n",
    "\n",
    "* The vectoriser .transform() methods:\n",
    "    - expect strings or list[strings] as input, \n",
    "    - and return Numpy ND arrays\n",
    "* The VectorStore .search() method expects:\n",
    "    - a string or list[strings] as input\n",
    "    - return a pandas dataframe of results with specific typed columns - ['query_id', 'query_text', 'doc_id', 'doc_text', 'rank', 'score']\n",
    "\n",
    "\n",
    "This expected typing ensures that the modules of the Package can work together, from vectorising, to searching, to deploying with FastAPI where the strict type modelling is used to generate the correct JSON schema format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<images of the type flow process go here?>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But sometimes you might want to perform additional operations on this data\n",
    "\n",
    "For example you might want your classification system to:\n",
    "\n",
    "* Convert all the text input to CAPITAL letters before it is embedded by the vectoriser,\n",
    "* Remove punctuation or do other sanitization on your input queries to a Vectorstore.search() method,\n",
    "* Remove any duplicate rows from a dataframe of results returned by a VectorStore.search call for a user query, based on a specific column in the data,\n",
    "* Perform spell checking before before a user's input query is passed to the Vectorstore.search(),\n",
    "* Inject some additional data into your classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Demo:\n",
    "\n",
    "This notebook demo will show you how to make:\n",
    "- a pre-processing function that removes punctuation from input user queries,\n",
    "- a post-processing function removes results rows that have duplicate ids to other rows of the results.\n",
    "\n",
    "- We will then make a final post-processing function that injects additional SOC definition data to the VectorStore results dataframe and show how this can be chained together with the deduplication code, to make a multi-step post-processing function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisite\n",
    "\n",
    "If you are new to the package, its recommended to follow through the ```general_workflow.ipynb``` notebook tutorial first. That interactive DEMO will showcase the core features of the ```ClassifAI package```. This currentl notebook provides examples of how to modify the flow of data which is initially described in the general_workflow.ipynb notebook.\n",
    "\n",
    "Check out the ClassifAI repository DEMO folder for all our notebook walkthrough tutorials including those mentioned above:\n",
    "\n",
    "https://github.com/datasciencecampus/classifai/tree/main/DEMO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if using pip\n",
    "# % pip install -e \".[huggingface]\"\n",
    "\n",
    "## if using uv\n",
    "# ! uv sync --extra huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal vectorstore setup\n",
    "\n",
    "We can start by loading a normal vectorstore up with no additional preprocessing. We can use one of our fake example known datasets is known to have several rows of data with the same ID value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classifai.indexers import VectorStore\n",
    "from classifai.vectorisers import HuggingFaceVectoriser\n",
    "\n",
    "vectoriser = HuggingFaceVectoriser(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "my_vector_store = VectorStore(\n",
    "    file_name=\"data/fake_soc_dataset.csv\",\n",
    "    data_type=\"csv\",\n",
    "    vectoriser=vectoriser,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice how when we run the search method with a user query:\n",
    " * an exclaimation mark in the query (that in some cases we may want to sanitise) is shown in the results. \n",
    " * Also the results for the below query should also show several rows with the same ```'doc_id'``` value (because our example data file had multiple entries with the same id label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store.search(\"a fruit and vegetable farmer!!!\", n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making pre- and post- processing hooks \n",
    "\n",
    "So lets write some functions that will remove punctuation on the user's input query, before the Vectorstore.search() method begins, and remove rows with duplicate IDs from the results dataframe just before the results are retutned from the Vectorstore.search() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def remove_punctuation(input_data):\n",
    "    # we wwant to modify the 'texts' field in the input_data pydantic model, which is a list of texts\n",
    "    # this line removes punctuation from each string with list comprehension\n",
    "    sanitized_texts = [x.translate(str.maketrans(\"\", \"\", string.punctuation)) for x in input_data.query]\n",
    "\n",
    "    input_data.query = sanitized_texts\n",
    "\n",
    "    # Return the modified input as a dictionary using model_dump from Pydantic\n",
    "    return input_data.model_dump()\n",
    "\n",
    "\n",
    "def drop_duplicates(expected_results_dataframe):\n",
    "    # we want to depuplicate the ranking attribute of the pydantic model which is a pandas dataframe\n",
    "    # specifically we want to drop all but the first occurrence of each unique 'doc_id' value for each subset of query results\n",
    "    expected_results_dataframe = expected_results_dataframe.drop_duplicates(subset=[\"query_id\", \"doc_id\"], keep=\"first\")\n",
    "\n",
    "    return expected_results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding our Hooks to the VectorStore\n",
    "\n",
    "Now when we initialise the Vectorstore we can declare our custom functions in the hooks dictionary.\n",
    "\n",
    "The Vectorstore codebase looks for specifically named dictionary entries in the Hooks dictionary, to decide what pre and post processing hooks to run. There are hooks for each major methods of the Vectoriser and VectorStore classes.\n",
    "\n",
    "Each dictionary entry uses the method name of the class and '_preprocessor' or '_postprocessor' appended to the name. Currenlty the implemented method hooks are:\n",
    "\n",
    "- for the vectoriser classes:\n",
    "    * tranform_preprocess\n",
    "    * transform_postprocess\n",
    "\n",
    "- for the VectorStore class:\n",
    "    * search_preprocess\n",
    "    * search_postprocess\n",
    "    * reverse_search_preprocess\n",
    "    * reverse_search_postprocess\n",
    "\n",
    "\n",
    "For our case in this excercise, we are implementig the search_preprocessor and search_postprocessor methods in the VectorStore.\n",
    "\n",
    "\n",
    "However if we preferred wanted to add a preprocessing or postprocessing hook to a Vectoriser we would pass that to the Vectoriser object on its instantiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store_with_hooks = VectorStore(\n",
    "    file_name=\"data/fake_soc_dataset.csv\",\n",
    "    data_type=\"csv\",\n",
    "    vectoriser=vectoriser,\n",
    "    overwrite=True,\n",
    "    hooks={\n",
    "        \"search_preprocess\": remove_punctuation,\n",
    "        \"search_postprocess\": drop_duplicates,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our hooks will run with the VectorStore search method\n",
    "\n",
    "\n",
    "Now we've passed our desired additional functions to our VectorStore initiation and those hook should run accordingly - lets see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store_with_hooks.search(\"a fruit and vegetable farmer!!!\", n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oops!\n",
    "\n",
    "Notice how in the above dataframe, the rank column now leaps over some values in each ranking. \n",
    "\n",
    "We didn't reset the ranking values, per query, when we removed duplicate rows...\n",
    "\n",
    "lets redo that now in a new function and hook it up to our preprocessing hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates_and_reset_rank(expected_results_dataframe):\n",
    "    # Remove duplicates based on 'query_id' and 'doc_id'\n",
    "    expected_results_dataframe = expected_results_dataframe.drop_duplicates(subset=[\"query_id\", \"doc_id\"], keep=\"first\")\n",
    "\n",
    "    # Reset the rank column per query_id using .loc to avoid SettingWithCopyWarning\n",
    "    expected_results_dataframe.loc[:, \"rank\"] = expected_results_dataframe.groupby(\"query_id\").cumcount()\n",
    "\n",
    "    return expected_results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and lets access the hooks directly from the vector store instance to modify them\n",
    "my_vector_store_with_hooks.hooks[\"search_postprocess\"] = drop_duplicates_and_reset_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### done - now lets run that query again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store_with_hooks.search(\"a fruit and vegetable farmer!!!\", n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This of course still works well when you pass multiple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store_with_hooks.search([\"a fruit and vegetable farmer!!!\", \"Digital marketing@\"], n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injecting Data into our classification results with a hook\n",
    "\n",
    "What if we had some additional context information that we wanted to add in our pipeline. It could be some official taxonomy definitions about our doc_id labels, such as SIC or SOC code definitions.\n",
    "\n",
    "We may want to inject this extra information that's not directly stored as metadata in the knowledgebase, so that a downstream component (such as a RAG agent) can use the additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But we also want keep our existing hook logic that removes punctuation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_id_definitions = {\n",
    "    \"101\": \"Fruit farmer: Grows and harvests fruits such as apples, oranges, and berries.\",\n",
    "    \"102\": \"iry farmer: Manages cows for milk production and processes dairy products.\",\n",
    "    \"103\": \"nstruction laborer: Performs physical tasks on construction sites, such as digging and carrying materials.\",\n",
    "    \"104\": \"rpenter: Constructs, installs, and repairs wooden frameworks and structures.\",\n",
    "    \"105\": \"ectrician: Installs, maintains, and repairs electrical systems in buildings and equipment.\",\n",
    "    \"106\": \"umber: Installs and repairs water, gas, and drainage systems in homes and businesses.\",\n",
    "    \"107\": \"ftware developer: Designs, writes, and tests computer programs and applications.\",\n",
    "    \"108\": \"ta analyst: Analyzes data to provide insights and support decision-making.\",\n",
    "    \"109\": \"countant: Prepares and examines financial records, ensuring accuracy and compliance with regulations.\",\n",
    "    \"110\": \"acher: Educates students in schools, colleges, or universities.\",\n",
    "    \"111\": \"rse: Provides medical care and support to patients in hospitals, clinics, or homes.\",\n",
    "    \"112\": \"ef: Prepares and cooks meals in restaurants, hotels, or other food establishments.\",\n",
    "    \"113\": \"aphic designer: Creates visual concepts for advertisements, websites, and branding.\",\n",
    "    \"114\": \"chanic: Repairs and maintains vehicles and machinery.\",\n",
    "    \"115\": \"otographer: Captures images for events, advertising, or artistic purposes.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_id_definitions(expected_results_dataframe):\n",
    "    # Map the 'doc_id' column to the corresponding definitions from the dictionary\n",
    "    expected_results_dataframe.loc[:, \"id_definition\"] = expected_results_dataframe[\"doc_id\"].map(\n",
    "        official_id_definitions\n",
    "    )\n",
    "\n",
    "    return expected_results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now combine this with our deduplicating hook in a new function that runs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(expected_results_dataframe):\n",
    "    # First, remove duplicates and reset rank\n",
    "    processed_dataframe = drop_duplicates_and_reset_rank(expected_results_dataframe)\n",
    "\n",
    "    # Then, add ID definitions\n",
    "    processed_dataframe = add_id_definitions(processed_dataframe)\n",
    "\n",
    "    # Return the final processed dataframe\n",
    "    return processed_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets once again update the postprocessing hook on our vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store_with_hooks.hooks[\"search_postprocess\"] = process_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and lets try the search again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vector_store_with_hooks.search([\"a fruit and vegetable farmer!!!\", \"Digital marketing@\"], n_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see a few NaN values because we did not provide definitions for all doc_ids in the dictionary for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    "\n",
    "- We wrote and combined several hooks on the Vectorstore class to:\n",
    "    - remove punctuation from queries before the  ```VectorStore.search()``` method is executed\n",
    "    - remove duplicates from the results list per query ranking and fixed the ranking\n",
    "    - injected data into our dataflow outside of constructing a vectorstore\n",
    "    - chained several Vectorstore.search() postprocessing steps together into one function that calls other functions\n",
    "\n",
    "- In this scenario we effectively showed how to deduplicate the rows of the results dataframe and add additional context columns of information in the form of the id_definitions. Hopefully, it is clear that you can add many pre- or post-processing steps this way, or by writing all steps in one big function - Hooks give you the flexibility and choice here.\n",
    "\n",
    "- Hooks let you disrupt the normal flow of data between Vectoriser, VectorStores and the RestAPI system. In this case we just had a small amount of dictionary data being added in, however the Hooks allow for more complex scenarios:\n",
    "    - using a 3rd party API to do automated corrective spell checking before passing your queries to the search method\n",
    "    - making an SQL query call to a database to get the extra information you want to inject in each row\n",
    "    - handle errors qhwn the API or database fails and just return the original Pydantic object or throw an error if needed\n",
    "\n",
    "\n",
    "### Next Steps and Challenges\n",
    "\n",
    "#### We focused soley on showcasing pre- and post-processing hooks for the VectorStore in this notebook:\n",
    "\n",
    "- See if you can implement some pre- and post- processing hooks for the Vectoriser class of you choice:\n",
    "    - try doing punctuation removal in the Vectoriser ```transform_preprocess()``` method and see how it is subtly different from performing this in the ```search_preprocess()``` hook of the VectorStore\n",
    "    - you could also try normalising the Numpy arrays returned by the vectoriser so that in the vectorstore construction and search your vectors will all be normalised and the search results will all contain values between 0 and 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
